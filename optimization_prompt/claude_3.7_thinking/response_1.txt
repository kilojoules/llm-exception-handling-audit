# Wind Farm Layout Optimization: HDF5 File Error Analysis

The error occurs during the merging phase of worker HDF5 files, with a specific error indicating a corrupted or empty file:

```
OSError: Unable to synchronously open file (file signature not found)
```

## Root Cause

Looking at the directory listing from `context_3.txt`, I spotted the problem:

```
0 -rw-rw-r--  1 juqu juqu    0 Feb 25 11:09 worker_61817.h5
```

There's a **zero-byte** worker file that's causing the failure. When the main process tries to read this file during the merging phase, it crashes because the file doesn't contain a valid HDF5 signature.

## How to Fix the Issue

The problem is in the `main()` function where worker files are merged without proper error handling:

```python
# Copy data from worker files
for worker_file in glob.glob(os.path.join(temp_dir, "worker_*.h5")):
    with h5py.File(worker_file, 'r') as h5_in:  # This line fails on invalid files
        for key in h5_in:
            if key != 'init' and key not in h5_out:  
                h5_out.copy(h5_in[key], key)
    # Clean up worker file
    os.remove(worker_file)
```

Here's a robust solution:

'''
# Modify the worker_init function to add validation
def worker_init(output_dir):
    """Initialize per-worker temporary HDF5 file"""
    import os
    import logging
    import h5py
    import numpy as np
    from multiprocessing import current_process
    
    logger = logging.getLogger(__name__)
    worker_id = current_process().pid
    worker_file = os.path.join(output_dir, f"worker_{worker_id}.h5")
    
    # Store file path on worker process object
    current_process().worker_file = worker_file
    
    # Initialize empty HDF5 file with a test dataset to ensure it's valid
    try:
        with h5py.File(worker_file, 'w') as f:
            f.create_dataset('init', data=np.array([1]))
            # Flush to ensure data is written to disk
            f.flush()
            logger.info(f"Worker {worker_id}: Successfully initialized HDF5 file")
    except Exception as e:
        logger.error(f"Worker {worker_id}: Failed to create HDF5 file: {str(e)}")
        # Re-raise to prevent worker from continuing with invalid file
        raise

    # Force process affinity to separate cores (Linux only)
    try:
        import psutil
        process = psutil.Process()
        worker_idx = int(current_process().name.split('-')[-1]) if '-' in current_process().name else 0
        process.cpu_affinity([worker_idx % os.cpu_count()])
        logger.info(f"Worker {worker_id} assigned to CPU {worker_idx % os.cpu_count()}")
    except Exception as e:
        logger.warning(f"Worker {worker_id}: CPU affinity setup failed: {str(e)}")


# Add a function to validate HDF5 files
def is_valid_hdf5(filepath, min_size_bytes=800):
    """
    Check if a file is a valid HDF5 file and meets minimum size requirements
    
    Args:
        filepath: Path to the HDF5 file
        min_size_bytes: Minimum expected file size in bytes
        
    Returns:
        bool: True if file is valid, False otherwise
    """
    import os
    import h5py
    
    # Check if file exists
    if not os.path.exists(filepath):
        return False
    
    # Check minimum file size
    if os.path.getsize(filepath) < min_size_bytes:
        return False
    
    # Try to open as HDF5
    try:
        with h5py.File(filepath, 'r') as _:
            pass
        return True
    except Exception:
        return False


# Modify the merging part of the main function
def main():
    # ... (rest of the code remains the same)
    
    # Merge worker files into final output
    if not args.no_hot_start and os.path.exists(args.output):
        # Use append mode to preserve existing results
        mode = 'a'
    else:
        # Use write mode for new file or when hot start is disabled
        mode = 'w'
    
    successful_merges = 0
    failed_merges = 0
    
    with h5py.File(args.output, mode) as h5_out:
        # Add metadata
        h5_out.attrs['grid_size'] = args.grid_size
        h5_out.attrs['random_pct'] = args.random_pct
        h5_out.attrs['total_configs'] = len(configs) + len(completed_configs)
        h5_out.attrs['successful_configs'] = success_count + len(completed_configs)
        
        # Copy data from worker files
        worker_files = glob.glob(os.path.join(temp_dir, "worker_*.h5"))
        logger.info(f"Found {len(worker_files)} worker files to merge")
        
        for worker_file in worker_files:
            # Validate file before attempting to open
            if not is_valid_hdf5(worker_file):
                logger.warning(f"Skipping invalid or empty worker file: {worker_file}")
                failed_merges += 1
                # Remove invalid file
                try:
                    os.remove(worker_file)
                except Exception as e:
                    logger.error(f"Failed to remove invalid file {worker_file}: {str(e)}")
                continue
                
            try:
                with h5py.File(worker_file, 'r') as h5_in:
                    keys_copied = 0
                    for key in h5_in:
                        if key != 'init' and key not in h5_out:  # Skip 'init' dataset and avoid duplicates
                            h5_out.copy(h5_in[key], key)
                            keys_copied += 1
                    
                    logger.info(f"Merged {keys_copied} configurations from {worker_file}")
                    successful_merges += 1
                    
                # Clean up worker file after successful merge
                os.remove(worker_file)
            except Exception as e:
                logger.error(f"Error merging {worker_file}: {str(e)}")
                failed_merges += 1
                # Don't delete file on error - keep for potential recovery
    
    logger.info(f"Merging complete: {successful_merges} successful, {failed_merges} failed")
    
    # Report if there were failures
    if failed_merges > 0:
        print(f"WARNING: {failed_merges} worker files could not be merged. See logs for details.", 
              file=sys.stderr, flush=True)
        print(f"Temporary files preserved in {temp_dir} for recovery.", file=sys.stderr, flush=True)
    else:
        # Only remove temp directory if all merges were successful
        try:
            os.rmdir(temp_dir)
        except Exception as e:
            logger.warning(f"Failed to remove temp directory: {str(e)}")
    
    logger.info(f"Results saved to {args.output}")
'''

## Key Improvements in the Solution

1. **Validation Function**: Added `is_valid_hdf5()` to check files before attempting to open them, verifying:
   - The file exists
   - The file size is at least the minimum expected (800 bytes based on valid examples)
   - The file can be opened as a valid HDF5 file

2. **Improved Worker Initialization**:
   - Added explicit file flush to ensure data is written to disk
   - Re-raising exceptions to prevent workers from continuing with invalid files

3. **Robust Merging Process**:
   - Pre-validates each worker file before attempting to open it
   - Skips invalid files rather than crashing
   - Counts successful and failed merges for reporting
   - Preserves failed files for possible data recovery

4. **Error Recovery Strategy**:
   - Only removes the temp directory if all merges were successful
   - Keeps problematic files for manual inspection
   - Provides clear warning messages to the user

5. **Better Logging**:
   - Comprehensive logging of the merging process
   - Clear indication of which files failed and why

## Additional Recommendations

1. **Periodic Validation**: Consider adding periodic validation of worker files during optimization to catch issues earlier.

2. **Worker Health Monitoring**: Implement a mechanism to detect and restart failed worker processes.

3. **Checkpointing**: Implement incremental merging of results to avoid losing everything if the final merge fails.

4. **Recovery Tool**: Create a dedicated recovery tool that can attempt to extract data from corrupted HDF5 files.

Would you like me to explain any part of this solution in more detail?
