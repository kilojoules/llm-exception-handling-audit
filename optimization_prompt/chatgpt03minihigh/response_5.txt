The error

```
OSError: Unable to synchronously open file (file signature not found)
```

usually means that one of your temporary worker files is either empty, incomplete, or corrupt. In your setup, each worker writes its results to a separate HDF5 file that is later merged into your final output file. If a worker file is invalid (for example, if the file was created but never properly written to), then attempting to open it with h5py will fail with an error like the one you’re seeing.

Below are several steps and code modifications to “fail fast” when a worker file is invalid and stop the overall process cleanly:

---

### 1. **Validate Worker Files Before Merging**

Before merging the worker files into your final HDF5 file, you can check for basic validity. One simple approach is to test the file size (an invalid file is more likely to be empty or under a minimum size) and then wrap the file open in a try/except block so that any error immediately leads to a controlled termination of your process.

For example, set a threshold (say, 1 KB) and then loop through your worker files:

```python
import sys

# Define a minimum valid file size in bytes (adjust as needed)
MIN_VALID_SIZE = 1024  

# Merge worker files into final output
if not args.no_hot_start and os.path.exists(args.output):
    mode = 'a'
else:
    mode = 'w'

with h5py.File(args.output, mode) as h5_out:
    h5_out.attrs['grid_size'] = args.grid_size
    h5_out.attrs['random_pct'] = args.random_pct
    h5_out.attrs['total_configs'] = len(configs) + len(completed_configs)
    h5_out.attrs['successful_configs'] = success_count + len(completed_configs)
    
    worker_files = glob.glob(os.path.join(temp_dir, "worker_*.h5"))
    for worker_file in worker_files:
        # Check file size first
        if os.path.getsize(worker_file) < MIN_VALID_SIZE:
            logger.error(f"Worker file {worker_file} is invalid (file size too small).")
            logger.error("Terminating process due to invalid worker file.")
            sys.exit(1)  # Exit the process cleanly
        
        try:
            with h5py.File(worker_file, 'r') as h5_in:
                for key in h5_in.keys():
                    # Skip the initialization dataset and avoid duplicates.
                    if key != 'init' and key not in h5_out:
                        h5_out.copy(h5_in[key], key)
        except Exception as e:
            logger.error(f"Error processing worker file {worker_file}: {e}")
            logger.error("Terminating process due to invalid worker file.")
            sys.exit(1)  # Exit immediately if any file is invalid
        finally:
            os.remove(worker_file)
```

**Explanation:**

- **File Size Check:**  
  Before even trying to open the worker file, we check if its size is above a preset threshold. A very small file (e.g., less than 1 KB) is a red flag that it might not contain the expected HDF5 structure.

- **Try/Except Block:**  
  When opening the file with `h5py.File()`, any error (like the “file signature not found” error) is caught immediately. We then log the error and terminate using `sys.exit(1)`, which stops the overall process.

- **Clean-up with `finally`:**  
  We still remove the worker file even if it’s invalid (you might decide to keep it for debugging purposes, though).

---

### 2. **Early Detection in Worker Initialization**

You already create an initialization dataset (`init`) when the worker file is created. You could also add a quick check right after the file creation to verify that the file has been written correctly. In your `worker_init()` function, you might add something like:

```python
def worker_init(output_dir):
    import os
    import logging
    import h5py
    import numpy as np
    from multiprocessing import current_process
    
    logger = logging.getLogger(__name__)
    worker_id = current_process().pid
    worker_file = os.path.join(output_dir, f"worker_{worker_id}.h5")
    
    current_process().worker_file = worker_file
    
    try:
        # Create and immediately close the file after writing a test dataset.
        with h5py.File(worker_file, 'w') as f:
            f.create_dataset('init', data=np.array([1]))
        # Basic validation: check file size after creation
        if os.path.getsize(worker_file) < 1024:
            raise IOError("Initialized worker file is too small, possibly not written correctly.")
        logger.info(f"Worker {worker_id}: Successfully initialized HDF5 file")
    except Exception as e:
        logger.error(f"Worker {worker_id}: Failed to create HDF5 file: {str(e)}")
        sys.exit(1)  # Exit the worker process (which might then signal to the pool)
    
    # Optional: setup CPU affinity as before
    try:
        import psutil
        process = psutil.Process()
        worker_idx = int(current_process().name.split('-')[-1]) if '-' in current_process().name else 0
        process.cpu_affinity([worker_idx % os.cpu_count()])
        logger.info(f"Worker {worker_id} assigned to CPU {worker_idx % os.cpu_count()}")
    except Exception as e:
        logger.warning(f"Worker {worker_id}: CPU affinity setup failed: {str(e)}")
```

**Explanation:**

- **Immediate File Validation:**  
  After creating the file, we immediately check the size. If the file is unusually small, an exception is raised and the worker process is terminated, preventing further work with an invalid file.

- **Process Exit in Worker Initialization:**  
  Exiting the worker (and thereby potentially failing the whole job early) is acceptable here if a fundamental issue is detected.

---

### 3. **Stopping the Process Cleanly**

By invoking `sys.exit(1)` as soon as an invalid file is encountered, you ensure that:
  
- The merging process does not continue with corrupt/incomplete data.
- You get a clean termination with a nonzero exit code indicating failure.
- Your logs immediately highlight which worker file was problematic, helping with debugging.

---

### Summary

- **Diagnose the Issue:**  
  The error stems from trying to open an invalid (possibly empty or improperly written) worker file.

- **Modify File Merging:**  
  Use a combination of file size checks and a try/except block around the `h5py.File()` open call. If a file does not pass validation, log the error and terminate the process cleanly using `sys.exit(1)`.

- **Improve Worker Initialization:**  
  Consider validating the worker file immediately upon creation in your `worker_init()` function.

Implementing these changes ensures that if any worker file is detected as invalid, the error is raised reliably and the script stops further processing—helping you avoid corrupt output in your final HDF5 file.

This modification will allow you to run your 50 seeds (or any number) while ensuring that any invalid worker file is caught early and the process is halted so you can debug and recover data without merging incomplete results.
