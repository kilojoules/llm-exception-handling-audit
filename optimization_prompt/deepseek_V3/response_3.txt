# Analysis of the HDF5 File Corruption Issue

From the error message and context files, I can see that the script failed during the final merge step when trying to combine worker HDF5 files into the main output file. The error occurred because one of the worker files (`worker_61817.h5`) was empty (0 bytes), causing HDF5 to fail when trying to read it.

## Root Cause

1. **Empty Worker File**: The `worker_61817.h5` file has 0 bytes, indicating it was either:
   - Never properly initialized
   - Got corrupted during writing
   - Was deleted/truncated during the process

2. **Insufficient Error Handling**: The script doesn't verify worker file integrity before attempting to merge them.

3. **Race Condition**: There might be a race condition where worker files are being read while still being written.

## Recommended Fixes

Here's how to modify the script to prevent this issue:

### 1. Add Worker File Validation

```python
def validate_worker_file(worker_file):
    """Validate that a worker file exists and is a valid HDF5 file"""
    if not os.path.exists(worker_file):
        return False
    if os.path.getsize(worker_file) == 0:
        return False
    try:
        with h5py.File(worker_file, 'r') as f:
            return True
    except:
        return False
```

### 2. Modify the Merge Section

Update the final merge section in `main()`:

```python
# Merge worker files into final output
if not args.no_hot_start and os.path.exists(args.output):
    mode = 'a'  # append mode
else:
    mode = 'w'  # write mode

with h5py.File(args.output, mode) as h5_out:
    # Add metadata
    h5_out.attrs['grid_size'] = args.grid_size
    h5_out.attrs['random_pct'] = args.random_pct
    h5_out.attrs['total_configs'] = len(configs) + len(completed_configs)
    h5_out.attrs['successful_configs'] = success_count + len(completed_configs)
    
    # Copy data from worker files
    valid_worker_files = []
    for worker_file in glob.glob(os.path.join(temp_dir, "worker_*.h5")):
        if validate_worker_file(worker_file):
            valid_worker_files.append(worker_file)
        else:
            logger.warning(f"Skipping invalid worker file: {worker_file}")
    
    if len(valid_worker_files) < len(glob.glob(os.path.join(temp_dir, "worker_*.h5"))):
        logger.error("Some worker files were invalid - results may be incomplete!")
    
    for worker_file in valid_worker_files:
        try:
            with h5py.File(worker_file, 'r') as h5_in:
                for key in h5_in:
                    if key != 'init' and key not in h5_out:
                        try:
                            h5_out.copy(h5_in[key], key)
                        except Exception as e:
                            logger.error(f"Failed to copy group {key} from {worker_file}: {e}")
            # Clean up worker file
            os.remove(worker_file)
        except Exception as e:
            logger.error(f"Failed to process worker file {worker_file}: {e}")
            continue
```

### 3. Add Recovery Mechanism

Add a function to recover data from partial runs:

```python
def recover_partial_results(output_file, temp_dir):
    """Attempt to recover results from partial runs"""
    if not os.path.exists(temp_dir):
        return False
    
    recovered = 0
    with h5py.File(output_file, 'a') as h5_out:
        for worker_file in glob.glob(os.path.join(temp_dir, "worker_*.h5")):
            if validate_worker_file(worker_file):
                try:
                    with h5py.File(worker_file, 'r') as h5_in:
                        for key in h5_in:
                            if key != 'init' and key not in h5_out:
                                h5_out.copy(h5_in[key], key)
                                recovered += 1
                    os.remove(worker_file)
                except:
                    continue
    
    if recovered > 0:
        logger.info(f"Recovered {recovered} configurations from partial run")
        return True
    return False
```

### 4. Modify Worker Initialization

Update `worker_init()` to ensure files are properly created:

```python
def worker_init(output_dir):
    """Initialize per-worker temporary HDF5 file"""
    worker_id = current_process().pid
    worker_file = os.path.join(output_dir, f"worker_{worker_id}.h5")
    
    # Ensure directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    # Store file path on worker process object
    current_process().worker_file = worker_file
    
    # Initialize empty HDF5 file with a test dataset
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            with h5py.File(worker_file, 'w') as f:
                f.create_dataset('init', data=np.array([1]))
                f.flush()  # Force write to disk
                os.fsync(f.fileno())  # Ensure physical write
                logger.info(f"Worker {worker_id}: Successfully initialized HDF5 file")
                break
        except Exception as e:
            if attempt == max_attempts - 1:
                logger.error(f"Worker {worker_id}: Failed to create HDF5 file after {max_attempts} attempts: {str(e)}")
                raise
            time.sleep(1)  # Wait before retrying
```

## How to Recover Your Data

1. First, try running the recovery function on your existing files:

```python
recover_partial_results("layouts.h5", "layouts.h5_temp")
```

2. Then check which configurations are still missing and run only those:

```python
# After recovery, check what's missing
completed_configs = scan_existing_results("layouts.h5")
all_configs = get_configs(50)  # 50 seeds
remaining_configs = filter_completed_configs(all_configs, completed_configs)

# Then run only the remaining configs
```

3. Consider adding checkpointing - save progress every N configurations rather than only at the end.

The key improvements are:
1. Better validation of worker files
2. More robust error handling during merge
3. Recovery mechanisms for partial runs
4. Safer file operations with retries and fsync
5. Better logging of issues

This should prevent the "file signature not found" errors and make the process more resilient to failures.
