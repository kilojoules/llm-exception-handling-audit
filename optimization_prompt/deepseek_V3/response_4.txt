# Analysis of the HDF5 File Corruption Issue

The error occurs during the final merging of worker files into the main output file, specifically when trying to read one of the worker files (`worker_*.h5`). The error message indicates the file signature wasn't found, suggesting file corruption or incomplete writes.

## Root Causes

1. **Incomplete Worker File Writes**: Some worker files may not have been properly closed after writing, leading to corruption.
2. **Race Conditions**: Multiple processes might have tried to write to the same file simultaneously.
3. **Disk I/O Issues**: Possible filesystem problems during writing.
4. **Process Termination**: Worker processes might have been terminated abruptly.

## Solution: Robust Worker File Handling

Here's how to modify the script to prevent and handle these issues:

### 1. Add Worker File Validation

```python
def validate_worker_file(worker_file):
    """Validate that a worker file is complete and readable"""
    try:
        with h5py.File(worker_file, 'r') as f:
            # Check basic structure
            if 'init' not in f:
                return False
            # Check at least one valid configuration group exists
            for key in f.keys():
                if isinstance(f[key], h5py.Group) and 'layout' in f[key] and 'aep' in f[key]:
                    return True
        return False
    except:
        return False
```

### 2. Modify Worker Initialization

```python
def worker_init(output_dir):
    """Initialize per-worker temporary HDF5 file with robust error handling"""
    import os
    import logging
    import h5py
    import numpy as np
    from multiprocessing import current_process
    
    logger = logging.getLogger(__name__)
    worker_id = current_process().pid
    worker_file = os.path.join(output_dir, f"worker_{worker_id}.h5")
    
    # Store file path on worker process object
    current_process().worker_file = worker_file
    
    # Initialize empty HDF5 file with atomic write
    temp_file = worker_file + '.tmp'
    try:
        with h5py.File(temp_file, 'w') as f:
            f.create_dataset('init', data=np.array([1]))
            f.flush()  # Force write to disk
        os.rename(temp_file, worker_file)  # Atomic operation
        logger.info(f"Worker {worker_id}: Successfully initialized HDF5 file")
    except Exception as e:
        logger.error(f"Worker {worker_id}: Failed to create HDF5 file: {str(e)}")
        if os.path.exists(temp_file):
            os.remove(temp_file)
        raise  # Re-raise to stop the process

    # Force process affinity to separate cores (Linux only)
    try:
        import psutil
        process = psutil.Process()
        worker_idx = int(current_process().name.split('-')[-1]) if '-' in current_process().name else 0
        process.cpu_affinity([worker_idx % os.cpu_count()])
        logger.info(f"Worker {worker_id} assigned to CPU {worker_idx % os.cpu_count()}")
    except Exception as e:
        logger.warning(f"Worker {worker_id}: CPU affinity setup failed: {str(e)}")
```

### 3. Modify the Optimization Function

```python
def optimize_layout(config, farm_boundaries, grid_size=18, random_pct=30, update_interval=None):
    """Run optimization with robust file handling"""
    try:
        worker_file = current_process().worker_file
        temp_file = worker_file + '.tmp'
        
        # ... [rest of your existing optimization code] ...
        
        # Save to temporary file first
        with h5py.File(temp_file, 'a') as f:
            grp = f.create_group(f"farm{farm_idx}_t{config['type_idx']}_s{config['ss_seed']}")
            grp.create_dataset('layout', data=np.vstack([problem['x'], problem['y']]))
            grp.create_dataset('aep', data=problem.cost) 
            for k, v in config.items():
                grp.attrs[k] = v
            f.flush()  # Force write to disk
        
        # Atomic rename
        if os.path.exists(worker_file):
            with h5py.File(worker_file, 'a') as f:
                with h5py.File(temp_file, 'r') as tmp_f:
                    for key in tmp_f.keys():
                        if key not in f:
                            f.copy(tmp_f[key], key)
            os.remove(temp_file)
        else:
            os.rename(temp_file, worker_file)
            
        logger.info(f"Successfully optimized farm {farm_idx}, type {config['type_idx']}, seed {config['ss_seed']}")
        return True

    except Exception as e:
        logger.error(f"Failed {config}: {str(e)}")
        if os.path.exists(temp_file):
            os.remove(temp_file)
        return False
    finally:
        gc.collect()
```

### 4. Modify the Main Function's Merge Logic

```python
def main():
    # ... [existing setup code] ...
    
    # Merge worker files into final output with validation
    valid_worker_files = []
    for worker_file in glob.glob(os.path.join(temp_dir, "worker_*.h5")):
        try:
            if validate_worker_file(worker_file):
                valid_worker_files.append(worker_file)
            else:
                logger.warning(f"Invalid worker file detected: {worker_file}")
                os.remove(worker_file)
        except Exception as e:
            logger.error(f"Error validating worker file {worker_file}: {e}")
            continue
    
    with h5py.File(args.output, mode) as h5_out:
        # Add metadata
        h5_out.attrs['grid_size'] = args.grid_size
        h5_out.attrs['random_pct'] = args.random_pct
        h5_out.attrs['total_configs'] = len(configs) + len(completed_configs)
        h5_out.attrs['successful_configs'] = success_count + len(completed_configs)
        
        # Copy data from valid worker files
        for worker_file in valid_worker_files:
            try:
                with h5py.File(worker_file, 'r') as h5_in:
                    for key in h5_in:
                        if key != 'init' and key not in h5_out:
                            h5_out.copy(h5_in[key], key)
                os.remove(worker_file)
            except Exception as e:
                logger.error(f"Failed to merge {worker_file}: {e}")
                continue
    
    # Clean up temp directory
    try:
        os.rmdir(temp_dir)
    except OSError:
        # Directory not empty - remove remaining files
        for f in glob.glob(os.path.join(temp_dir, "*")):
            try:
                os.remove(f)
            except:
                pass
        os.rmdir(temp_dir)
    
    logger.info(f"Results saved to {args.output}")
```

## Recovery Strategy for Existing Data

Since you have the worker files, you can recover the data by:

1. Create a new script to scan all worker files and merge valid ones:

```python
import h5py
import os
import glob
from collections import defaultdict

def recover_data(worker_files, output_file):
    """Recover data from worker files"""
    completed_configs = defaultdict(dict)
    
    for worker_file in worker_files:
        try:
            with h5py.File(worker_file, 'r') as f:
                for key in f:
                    if isinstance(f[key], h5py.Group) and 'layout' in f[key] and 'aep' in f[key]:
                        config_key = key
                        if config_key not in completed_configs[worker_file]:
                            completed_configs[worker_file][config_key] = {
                                'layout': f[key]['layout'][:],
                                'aep': f[key]['aep'][()],
                                'attrs': dict(f[key].attrs)
                            }
        except:
            continue
    
    # Write to new output file
    with h5py.File(output_file, 'w') as f_out:
        for worker, configs in completed_configs.items():
            for config_key, data in configs.items():
                grp = f_out.create_group(config_key)
                grp.create_dataset('layout', data=data['layout'])
                grp.create_dataset('aep', data=data['aep'])
                for k, v in data['attrs'].items():
                    grp.attrs[k] = v
    
    return len(completed_configs)

# Usage:
worker_files = glob.glob("layouts.h5_temp/worker_*.h5")
recovered = recover_data(worker_files, "recovered_layouts.h5")
print(f"Recovered {recovered} configurations")
```

## Prevention for Future Runs

1. **Use atomic writes**: Always write to temporary files first, then rename
2. **Add validation checks**: Verify files before merging
3. **Implement periodic merging**: Merge worker files periodically during long runs
4. **Add checksum verification**: For critical data files
5. **Use file locking**: If multiple processes might access the same files

These changes will make the script more robust against file corruption issues while maintaining the parallel processing benefits.
