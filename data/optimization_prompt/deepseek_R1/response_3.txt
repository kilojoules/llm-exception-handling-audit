To resolve the error caused by invalid worker files during the merge phase and ensure clean process termination, modify the script as follows:

**1. Enhance Worker File Initialization Validation**
```python
def worker_init(output_dir):
    """Initialize per-worker temporary HDF5 file with validation"""
    worker_id = current_process().pid
    worker_file = os.path.join(output_dir, f"worker_{worker_id}.h5")
    current_process().worker_file = worker_file

    try:
        # Validate file creation
        with h5py.File(worker_file, 'w') as f:
            f.create_dataset('init', data=np.array([1]))
            f.flush()  # Force write to disk
        logger.info(f"Worker {worker_id}: HDF5 file initialized")
        
        # Verify file can be reopened
        with h5py.File(worker_file, 'r') as f:
            assert 'init' in f, "Initialization dataset missing"
            
    except Exception as e:
        logger.critical(f"Worker {worker_id}: Failed to initialize HDF5 file: {str(e)}")
        os._exit(1)  # Force immediate termination of worker
```

**2. Add File Validation During Merge Phase**
```python
# In main() function, replace the merging block with:

# Copy data from worker files with validation
valid_workers = 0
for worker_file in glob.glob(os.path.join(temp_dir, "worker_*.h5")):
    try:
        # Basic file validity check
        if os.path.getsize(worker_file) < 100:  # Minimum expected size
            raise ValueError("File too small to be valid HDF5")
            
        # Try opening with HDF5
        with h5py.File(worker_file, 'r') as h5_in:
            if 'init' not in h5_in:
                raise ValueError("Missing initialization marker")
                
            # Copy contents
            for key in h5_in:
                if key != 'init' and key not in h5_out:
                    h5_out.copy(h5_in[key], key)
        valid_workers += 1
        
    except Exception as e:
        logger.error(f"Skipping invalid worker file {worker_file}: {str(e)}")
        continue
        
    # Clean up worker file regardless of validity
    try:
        os.remove(worker_file)
    except:
        pass

logger.info(f"Merged {valid_workers} valid worker files")
```

**3. Add Pre-run Cleanup**
```python
# At start of main() function:
# Clean up any previous temp files
if os.path.exists(temp_dir):
    shutil.rmtree(temp_dir, ignore_errors=True)
os.makedirs(temp_dir, exist_ok=True)
```

**4. Add Emergency Shutdown Mechanism**
```python
# Add global error tracking
error_flag = Value('b', False)

def worker_wrapper(config, *args, **kwargs):
    try:
        return optimize_layout(config, *args, **kwargs)
    except Exception as e:
        logger.critical(f"Critical error in worker: {str(e)}")
        error_flag.value = True
        raise

# In main() where pool is created:
with Pool(...) as pool:
    # ... existing code ...
    
    # Modified results collection
    results = []
    for result in iterator:
        if error_flag.value:
            logger.critical("Error flag detected. Terminating pool...")
            pool.terminate()
            break
        results.append(result)
        pbar.update(1)
```

**Key Improvements:**
1. Strict validation of worker files during initialization
2. File size and content checks before merging
3. Emergency termination on critical errors
4. Pre-run cleanup of temporary files
5. Proper error propagation between processes

**To Recover Existing Data:**
1. Manually inspect worker files:
```bash
h5check worker_*.h5  # Use h5check utility
```
2. Create a recovery script to copy valid groups:
```python
with h5py.File('recovered.h5', 'w') as out:
    for fname in glob.glob('worker_*.h5'):
        try:
            with h5py.File(fname, 'r') as src:
                for key in src:
                    if key not in out:
                        src.copy(key, out)
        except:
            continue
```

These modifications will ensure the process fails fast on HDF5 issues and provides better error diagnostics while maintaining data integrity.
